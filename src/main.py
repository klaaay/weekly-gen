#!/usr/bin/env python3
"""
Weekly Newsletter Scraper - ä¸»å…¥å£æ–‡ä»¶
ä¸€æ¬¡æ€§æ‰§è¡Œæ‰€æœ‰å‘¨åˆŠçˆ¬å–è„šæœ¬
"""
import os
import sys
import time
import concurrent.futures
import threading
from datetime import datetime
from typing import List, Tuple, Dict, Any

# å¯¼å…¥æ‰€æœ‰è„šæœ¬çš„ä¸»å‡½æ•°
try:
    from src.frontendfoc import scrape_frontendfoc
    from src.javascriptweekly import scrape_javascriptweekly
    from src.leadershipintech import scrape_leadershipintech
    from src.nextjsweekly import scrape_nextjsweekly
    from src.nodeweekly import scrape_nodeweekly
    from src.programmingdigest import scrape_programmingdigest
    from src.reactweekly import scrape_reactweekly
    from src.reactdigest import scrape_reactdigest
    from src.thisweekinreact import scrape_thisweekinreact
    from src.webtoolsweekly import scrape_webtoolsweekly
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯ï¼š{e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰è„šæœ¬æ–‡ä»¶éƒ½åœ¨å½“å‰ç›®å½•ä¸‹")
    sys.exit(1)

# å®šä¹‰æ‰€æœ‰å¯ç”¨çš„è„šæœ¬
SCRAPERS = {
    'frontendfoc': {
        'name': 'Frontend Focus',
        'function': scrape_frontendfoc,
        'description': 'Frontend å¼€å‘ç›¸å…³å‘¨åˆŠ'
    },
    'javascriptweekly': {
        'name': 'JavaScript Weekly',
        'function': scrape_javascriptweekly,
        'description': 'JavaScript å¼€å‘å‘¨åˆŠ'
    },
    'leadershipintech': {
        'name': 'Leadership in Tech',
        'function': scrape_leadershipintech,
        'description': 'æŠ€æœ¯é¢†å¯¼åŠ›å‘¨åˆŠ'
    },
    'nextjsweekly': {
        'name': 'Next.js Weekly',
        'function': scrape_nextjsweekly,
        'description': 'Next.js æ¡†æ¶å‘¨åˆŠ'
    },
    'nodeweekly': {
        'name': 'Node Weekly',
        'function': scrape_nodeweekly,
        'description': 'Node.js å¼€å‘å‘¨åˆŠ'
    },
    'programmingdigest': {
        'name': 'Programming Digest',
        'function': scrape_programmingdigest,
        'description': 'ç¼–ç¨‹æŠ€æœ¯æ–‡æ‘˜'
    },
    'reactweekly': {
        'name': 'React Weekly',
        'function': scrape_reactweekly,
        'description': 'React å¼€å‘å‘¨åˆŠ'
    },
    'reactdigest': {
        'name': 'React Digest',
        'function': scrape_reactdigest,
        'description': 'React æŠ€æœ¯æ–‡æ‘˜'
    },
    'thisweekinreact': {
        'name': 'This Week in React',
        'function': scrape_thisweekinreact,
        'description': 'React ç”Ÿæ€ç³»ç»Ÿå‘¨åˆŠ'
    },
    'webtoolsweekly': {
        'name': 'Web Tools Weekly',
        'function': scrape_webtoolsweekly,
        'description': 'Web å¼€å‘å·¥å…·å‘¨åˆŠ'
    }
}

# çº¿ç¨‹å®‰å…¨çš„æ‰“å°é”
print_lock = threading.Lock()

def safe_print(message: str):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°å‡½æ•°"""
    with print_lock:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {message}")

def run_scraper(scraper_key: str, scraper_info: Dict[str, Any]) -> Tuple[str, bool, str]:
    """
    è¿è¡Œå•ä¸ªçˆ¬è™«è„šæœ¬
    
    Args:
        scraper_key: è„šæœ¬æ ‡è¯†ç¬¦
        scraper_info: è„šæœ¬ä¿¡æ¯å­—å…¸
    
    Returns:
        Tuple[è„šæœ¬åç§°ï¼Œæ˜¯å¦æˆåŠŸï¼Œç»“æœæ¶ˆæ¯]
    """
    scraper_name = scraper_info['name']
    scraper_func = scraper_info['function']
    
    try:
        safe_print(f"å¼€å§‹æ‰§è¡Œï¼š{scraper_name}")
        start_time = time.time()
        
        # æ‰§è¡Œçˆ¬è™«å‡½æ•°
        scraper_func()
        
        end_time = time.time()
        duration = end_time - start_time
        
        success_msg = f"âœ… {scraper_name} æ‰§è¡ŒæˆåŠŸ (è€—æ—¶ï¼š{duration:.2f}ç§’)"
        safe_print(success_msg)
        return scraper_key, True, success_msg
        
    except Exception as e:
        error_msg = f"âŒ {scraper_name} æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"
        safe_print(error_msg)
        return scraper_key, False, error_msg

def run_all_scrapers(max_workers: int = 4, selected_scrapers: List[str] = None) -> Dict[str, Any]:
    """
    å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰çˆ¬è™«è„šæœ¬
    
    Args:
        max_workers: æœ€å¤§å¹¶å‘æ•°
        selected_scrapers: æŒ‡å®šè¦æ‰§è¡Œçš„çˆ¬è™«åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºæ‰§è¡Œå…¨éƒ¨
    
    Returns:
        æ‰§è¡Œç»“æœç»Ÿè®¡
    """
    # ç¡®å®šè¦æ‰§è¡Œçš„çˆ¬è™«
    if selected_scrapers:
        scrapers_to_run = {k: v for k, v in SCRAPERS.items() if k in selected_scrapers}
    else:
        scrapers_to_run = SCRAPERS
    
    safe_print(f"å‡†å¤‡æ‰§è¡Œ {len(scrapers_to_run)} ä¸ªçˆ¬è™«è„šæœ¬ï¼Œæœ€å¤§å¹¶å‘æ•°ï¼š{max_workers}")
    safe_print("=" * 60)
    
    results = []
    start_time = time.time()
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_scraper = {
            executor.submit(run_scraper, key, info): key 
            for key, info in scrapers_to_run.items()
        }
        
        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_scraper):
            scraper_key = future_to_scraper[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                error_msg = f"âŒ {SCRAPERS[scraper_key]['name']} æ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}"
                safe_print(error_msg)
                results.append((scraper_key, False, error_msg))
    
    end_time = time.time()
    total_duration = end_time - start_time
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for _, success, _ in results if success)
    failed = len(results) - successful
    
    safe_print("=" * 60)
    safe_print(f"æ‰§è¡Œå®Œæˆï¼æ€»è€—æ—¶ï¼š{total_duration:.2f}ç§’")
    safe_print(f"æˆåŠŸï¼š{successful} ä¸ªï¼Œå¤±è´¥ï¼š{failed} ä¸ª")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    safe_print("\nğŸ“Š è¯¦ç»†ç»“æœï¼š")
    for scraper_key, success, message in results:
        safe_print(f"  {message}")
    
    return {
        'total': len(results),
        'successful': successful,
        'failed': failed,
        'duration': total_duration,
        'results': results
    }

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("Weekly Newsletter Scraper - ä½¿ç”¨è¯´æ˜")
    print("=" * 50)
    print("ç”¨æ³•ï¼špython main.py [é€‰é¡¹] [è„šæœ¬åç§°...]")
    print()
    print("é€‰é¡¹ï¼š")
    print("  -h, --help          æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
    print("  -l, --list          åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è„šæœ¬")
    print("  -w, --workers N     è®¾ç½®æœ€å¤§å¹¶å‘æ•° (é»˜è®¤ï¼š4)")
    print("  -a, --all           æ‰§è¡Œæ‰€æœ‰è„šæœ¬ (é»˜è®¤)")
    print()
    print("è„šæœ¬åç§°ï¼š")
    for key, info in SCRAPERS.items():
        print(f"  {key:<20} {info['description']}")
    print()
    print("ç¤ºä¾‹ï¼š")
    print("  python main.py                           # æ‰§è¡Œæ‰€æœ‰è„šæœ¬")
    print("  python main.py frontendfoc nodeweekly    # åªæ‰§è¡ŒæŒ‡å®šè„šæœ¬")
    print("  python main.py -w 8 --all               # ä½¿ç”¨ 8 ä¸ªå¹¶å‘æ‰§è¡Œæ‰€æœ‰è„šæœ¬")

def list_scrapers():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è„šæœ¬"""
    print("å¯ç”¨çš„çˆ¬è™«è„šæœ¬ï¼š")
    print("=" * 50)
    for key, info in SCRAPERS.items():
        print(f"ğŸ”¹ {key}")
        print(f"   åç§°ï¼š{info['name']}")
        print(f"   æè¿°ï¼š{info['description']}")
        print()

def main():
    """ä¸»å‡½æ•°"""
    args = sys.argv[1:]
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    max_workers = 4
    selected_scrapers = []
    show_all = False
    
    i = 0
    while i < len(args):
        arg = args[i]
        
        if arg in ['-h', '--help']:
            show_help()
            return
        elif arg in ['-l', '--list']:
            list_scrapers()
            return
        elif arg in ['-w', '--workers']:
            if i + 1 < len(args):
                try:
                    max_workers = int(args[i + 1])
                    i += 1
                except ValueError:
                    print(f"é”™è¯¯ï¼šæ— æ•ˆçš„å¹¶å‘æ•° '{args[i + 1]}'")
                    return
            else:
                print("é”™è¯¯ï¼š--workers éœ€è¦æŒ‡å®šæ•°å€¼")
                return
        elif arg in ['-a', '--all']:
            show_all = True
        elif arg.startswith('-'):
            print(f"é”™è¯¯ï¼šæœªçŸ¥é€‰é¡¹ '{arg}'")
            show_help()
            return
        else:
            # éªŒè¯è„šæœ¬åç§°æ˜¯å¦æœ‰æ•ˆ
            if arg in SCRAPERS:
                selected_scrapers.append(arg)
            else:
                print(f"é”™è¯¯ï¼šæœªçŸ¥è„šæœ¬ '{arg}'")
                print("ä½¿ç”¨ -l æŸ¥çœ‹æ‰€æœ‰å¯ç”¨è„šæœ¬")
                return
        
        i += 1
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè„šæœ¬ä¸”æ²¡æœ‰ä½¿ç”¨ --allï¼Œé»˜è®¤æ‰§è¡Œæ‰€æœ‰è„šæœ¬
    if not selected_scrapers and not show_all:
        selected_scrapers = None  # æ‰§è¡Œæ‰€æœ‰è„šæœ¬
    elif show_all:
        selected_scrapers = None  # æ‰§è¡Œæ‰€æœ‰è„šæœ¬
    
    # æ‰§è¡Œçˆ¬è™«
    try:
        results = run_all_scrapers(max_workers, selected_scrapers)
        
        # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
        if results['failed'] > 0:
            sys.exit(1)  # æœ‰å¤±è´¥çš„è„šæœ¬
        else:
            sys.exit(0)  # å…¨éƒ¨æˆåŠŸ
            
    except KeyboardInterrupt:
        safe_print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(130)
    except Exception as e:
        safe_print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼š{str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
